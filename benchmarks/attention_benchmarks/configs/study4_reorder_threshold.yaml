# Study 4: What is optimal reorder_batch_threshold for MLA backends supporting query length > 1?
# Question: At what query length does prefill pipeline become faster than decode pipeline?
# Methodology: For each query length, compare decode vs prefill performance to find crossover point
# Applies to: FlashAttn MLA, FlashMLA

description: "Decode vs Prefill pipeline crossover analysis"

# Test FlashAttn MLA (recommended - FlashMLA has known issues with speculative decode)
backend: flash_attn_mla

# Mode: decode_vs_prefill comparison (special sweep mode)
# For each batch spec, we'll test both decode and prefill pipelines
mode: "decode_vs_prefill"

# Query lengths to test (from old benchmark_mla_threshold.py methodology)
# Each query length will be tested with BOTH decode and prefill pipelines:
#   - decode: threshold >= query_length (forces decode pipeline)
#   - prefill: threshold < query_length (forces prefill pipeline)
#
# We use spec<N>s1k format which creates q_len=N, kv_len=1024 requests
# This tests different query lengths with fixed KV cache context
batch_specs:
  # Fine-grained: 1-16 (decode range, step 1)
  - "q1kv1k"         # q_len=1 (regular decode, not spec)
  - "q2kv1k"    # q_len=2
  - "q3kv1k"    # q_len=3
  - "q4kv1k"    # q_len=4
  - "q5kv1k"    # q_len=5
  - "q6kv1k"    # q_len=6
  - "q7kv1k"    # q_len=7
  - "q8kv1k"    # q_len=8
  - "q9kv1k"    # q_len=9
  - "q10kv1k"   # q_len=10
  - "q11kv1k"   # q_len=11
  - "q12kv1k"   # q_len=12
  - "q13kv1k"   # q_len=13
  - "q14kv1k"   # q_len=14
  - "q15kv1k"   # q_len=15
  - "q16kv1k"   # q_len=16
  # Transition zone: 17-64 (step 2)
  - "q17kv1k"
  - "q19kv1k"
  - "q21kv1k"
  - "q23kv1k"
  - "q25kv1k"
  - "q27kv1k"
  - "q29kv1k"
  - "q31kv1k"
  - "q33kv1k"
  - "q35kv1k"
  - "q37kv1k"
  - "q39kv1k"
  - "q41kv1k"
  - "q43kv1k"
  - "q45kv1k"
  - "q47kv1k"
  - "q49kv1k"
  - "q51kv1k"
  - "q53kv1k"
  - "q55kv1k"
  - "q57kv1k"
  - "q59kv1k"
  - "q61kv1k"
  - "q63kv1k"
  # Prefill range: 65-128 (step 4)
  - "q65kv1k"
  - "q69kv1k"
  - "q73kv1k"
  - "q77kv1k"
  - "q81kv1k"
  - "q85kv1k"
  - "q89kv1k"
  - "q93kv1k"
  - "q97kv1k"
  - "q101kv1k"
  - "q105kv1k"
  - "q109kv1k"
  - "q113kv1k"
  - "q117kv1k"
  - "q121kv1k"
  - "q125kv1k"

# Batch sizes to test (from old script)
batch_sizes:
  - 1
  - 2
  - 4
  - 8
  - 16
  - 32
  - 64
  - 128
  - 256

# Model configuration (DeepSeek V2/V3 defaults)
model:
  num_layers: 10
  head_dim: 576
  num_q_heads: 128
  num_kv_heads: 1
  block_size: 128

# Benchmark settings
benchmark:
  device: "cuda:0"
  repeats: 15          # More repeats for spec decode variance
  warmup_iters: 5
  profile_memory: false

# Output
output:
  csv: "study4_reorder_threshold_results.csv"
  json: "study4_reorder_threshold_results.json"

# Expected outcome (reproduces old benchmark_mla_threshold.py study):
# - For each batch size, find the crossover point where prefill becomes faster than decode
# - Show decode vs prefill performance across all query lengths (1-125)
# - Determine optimal reorder_batch_threshold based on last query length where decode is faster
# - Understand how crossover point varies with batch size
# - Provide data-driven guidance for default threshold value
#
# Methodology (from old script):
# - Each query length tested with BOTH pipelines:
#     * decode: threshold >= query_length (forces decode pipeline)
#     * prefill: threshold < query_length (forces prefill pipeline)
# - Compare which is faster to find crossover point
# - Use multiple repeats (15) to handle variance
#
# Note: FlashMLA may have issues with speculative decode workloads
# Use flash_attn_mla instead if you encounter errors with flashmla
