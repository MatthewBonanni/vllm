# Study 2: Does head count matter for FlashAttn MLA vs FlashMLA on Hopper?
# Question: Which backend performs better on Hopper GPUs (SM90+)?
# Question: Does the number of attention heads affect relative performance?

description: "FlashAttn MLA vs FlashMLA head count comparison on Hopper"

# Compare these two Hopper backends
backends:
  - flashattn_mla
  - flashmla

# Comprehensive batch spec matrix: batch sizes Ã— sequence lengths
# Batch sizes: 1, 2, 4, 8, 16, 24, 32, 48, 64, 96, 128
# Sequence lengths: 1k, 2k, 4k, 8k, 16k, 32k, 64k, 128k
batch_specs:
  # Batch size: 1
  - "1q1s1k"
  - "1q1s2k"
  - "1q1s4k"
  - "1q1s8k"
  - "1q1s16k"
  - "1q1s32k"
  - "1q1s64k"
  - "1q1s128k"

  # Batch size: 2
  - "2q1s1k"
  - "2q1s2k"
  - "2q1s4k"
  - "2q1s8k"
  - "2q1s16k"
  - "2q1s32k"
  - "2q1s64k"
  - "2q1s128k"

  # Batch size: 4
  - "4q1s1k"
  - "4q1s2k"
  - "4q1s4k"
  - "4q1s8k"
  - "4q1s16k"
  - "4q1s32k"
  - "4q1s64k"
  - "4q1s128k"

  # Batch size: 8
  - "8q1s1k"
  - "8q1s2k"
  - "8q1s4k"
  - "8q1s8k"
  - "8q1s16k"
  - "8q1s32k"
  - "8q1s64k"
  - "8q1s128k"

  # Batch size: 16
  - "16q1s1k"
  - "16q1s2k"
  - "16q1s4k"
  - "16q1s8k"
  - "16q1s16k"
  - "16q1s32k"
  - "16q1s64k"
  - "16q1s128k"

  # Batch size: 24
  - "24q1s1k"
  - "24q1s2k"
  - "24q1s4k"
  - "24q1s8k"
  - "24q1s16k"
  - "24q1s32k"
  - "24q1s64k"
  - "24q1s128k"

  # Batch size: 32
  - "32q1s1k"
  - "32q1s2k"
  - "32q1s4k"
  - "32q1s8k"
  - "32q1s16k"
  - "32q1s32k"
  - "32q1s64k"
  - "32q1s128k"

  # Batch size: 48
  - "48q1s1k"
  - "48q1s2k"
  - "48q1s4k"
  - "48q1s8k"
  - "48q1s16k"
  - "48q1s32k"
  - "48q1s64k"
  - "48q1s128k"

  # Batch size: 64
  - "64q1s1k"
  - "64q1s2k"
  - "64q1s4k"
  - "64q1s8k"
  - "64q1s16k"
  - "64q1s32k"
  - "64q1s64k"
  - "64q1s128k"

  # Batch size: 96
  - "96q1s1k"
  - "96q1s2k"
  - "96q1s4k"
  - "96q1s8k"
  - "96q1s16k"
  - "96q1s32k"
  - "96q1s64k"
  - "96q1s128k"

  # Batch size: 128
  - "128q1s1k"
  - "128q1s2k"
  - "128q1s4k"
  - "128q1s8k"
  - "128q1s16k"
  - "128q1s32k"
  - "128q1s64k"
  - "128q1s128k"

# Model configuration
model:
  num_layers: 10
  head_dim: 576        # MLA uses 576
  num_q_heads: 128     # Default value (will be overridden by sweep)
  num_kv_heads: 1      # MLA uses single KV head
  block_size: 128

# Model parameter sweep - test different head counts
model_parameter_sweep:
  param_name: "num_q_heads"
  values: [16, 32, 64, 128, 256]
  label_format: "{backend}_heads_{value}"

# Benchmark settings
benchmark:
  device: "cuda:0"
  repeats: 10
  warmup_iters: 5
  profile_memory: true  # Track memory usage differences

# Output
output:
  csv: "hopper_head_count_results.csv"
  json: "hopper_head_count_results.json"

# Expected outcome:
# - Determine which backend is faster on Hopper
# - Identify if head count impacts relative performance
# - Inform backend selection for DeepSeek V2/V3 models
