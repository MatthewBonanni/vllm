# Speculative decoding benchmark configuration
# Tests reorder_batch_threshold optimization

model:
  name: "deepseek-v3"
  num_layers: 60
  num_q_heads: 128
  num_kv_heads: 1
  head_dim: 576
  kv_lora_rank: 512
  qk_nope_head_dim: 128
  qk_rope_head_dim: 64
  v_head_dim: 128

batch_specs:
  # Pure speculative decode (K-token verification)
  - "q2kv1k"      # 2-token spec, 1k KV
  - "q4kv1k"      # 4-token spec, 1k KV
  - "q8kv1k"      # 8-token spec, 1k KV
  - "q16kv1k"     # 16-token spec, 1k KV

  # Speculative with different context lengths
  - "q4kv2k"      # 4-token spec, 2k KV
  - "q4kv4k"      # 4-token spec, 4k KV
  - "q8kv2k"      # 8-token spec, 2k KV
  - "q8kv4k"      # 8-token spec, 4k KV

  # Mixed: speculative + regular decode
  - "32q4kv1k"                    # 32 spec requests
  - "16q4kv1k_16q1kv1k"              # 16 spec + 16 regular
  - "8q8kv2k_24q1kv2k"               # 8 spec (8-tok) + 24 regular

  # Mixed: speculative + prefill + decode
  - "2q1k_16q4kv1k_16q1kv1k"         # 2 prefill + 16 spec + 16 decode
  - "4q2k_32q4kv2k_32q1kv2k"         # 4 prefill + 32 spec + 32 decode

  # Large batches with speculation
  - "64q4kv1k"                    # 64 spec requests
  - "32q8kv2k"                    # 32 spec (8-token)
  - "16q16kv4k"                   # 16 spec (16-token)

# Backends that support query length > 1
backends:
  - flash_attn_mla    # reorder_batch_threshold = 512
  - flashmla          # reorder_batch_threshold = 1 (tunable)

# FlashInfer-MLA also supports uniform spec-as-decode but with different mechanism
# - flashinfer_mla

device: "cuda:0"
repeats: 10  # More repeats for statistical significance
warmup_iters: 5
profile_memory: false

# Test these threshold values for optimization
reorder_batch_thresholds:
  - 1
  - 2
  - 4
  - 8
  - 16
  - 32
  - 64
  - 128
  - 256
  - 512
