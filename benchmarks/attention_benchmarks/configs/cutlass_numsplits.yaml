# Study 1: Should we revert CUTLASS MLA num-splits heuristic?
# Question: What is the optimal num_kv_splits for different batch sizes?
# Related PRs: #24966, #25509

description: "CUTLASS MLA num-splits optimization study"

# Single backend for this study
backend: cutlass_mla

# Test various decode batch sizes with different KV cache lengths
batch_specs:
  - "32q1s1k"     # 32 decode requests, 1k KV cache
  - "64q1s1k"     # 64 decode requests, 1k KV cache
  - "64q1s4k"     # 64 decode requests, 4k KV cache
  - "64q1s16k"    # 64 decode requests, 16k KV cache
  - "128q1s1k"    # 128 decode requests, 1k KV cache
  - "128q1s4k"    # 128 decode requests, 4k KV cache

# Sweep num_kv_splits values
num_splits:
  - 1
  - 2
  - 4
  - 8
  - 16
  - 32

# Compare against auto-selected num_kv_splits
compare_auto: false

# Model configuration (DeepSeek V2/V3 defaults)
model:
  num_layers: 10
  head_dim: 576        # MLA uses 576 (kv_lora_rank=512 + 64)
  num_q_heads: 128
  num_kv_heads: 1      # MLA uses single KV head
  block_size: 128

# Benchmark settings
benchmark:
  device: "cuda:0"
  repeats: 10          # More repeats for statistical significance
  warmup_iters: 5
  profile_memory: false

# Output
output:
  csv: "cutlass_numsplits_results.csv"
  json: "cutlass_numsplits_results.json"

# Expected outcome:
# - Identify if auto-selection heuristic is optimal
# - Determine if we should revert PRs #24966, #25509
# - Find optimal num_kv_splits per batch configuration
